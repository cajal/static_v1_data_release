{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8880444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading local settings from pipeline_config.json\n",
      "Connecting pfahey@at-database.ad.bcm.edu:3306\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "from itertools import compress\n",
    "from scipy.interpolate import InterpolatedUnivariateSpline, interp1d\n",
    "\n",
    "import datajoint as dj\n",
    "import static_nda as nda\n",
    "from stimulus import stimulus\n",
    "from pipeline import meso, fuse,treadmill\n",
    "pupil = dj.create_virtual_module(\"pupil\", \"pipeline_eye\")\n",
    "anatomy = dj.create_virtual_module(\"anatomy\", \"pipeline_anatomy\")\n",
    "neuro_data = dj.create_virtual_module('neuro_data','neurodata_static')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d6fa46",
   "metadata": {},
   "source": [
    "Reformatted to consolidate only data processing steps applied for static release dataset out of subfunctions into single notebook.  Removed all references to tiering, which depend on ConditionTier table in at-fabee.ad.bcm.edu database. \n",
    "\n",
    "__Please note that the cells in this notebook are interdependent and may reuse variables, and should be run in order.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fbde3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_key  = {'animal_id': 21067, 'session': 10, 'scan_idx': 18} \n",
    "\n",
    "# config parameters used for static release dataset\n",
    "_valid_types = [\n",
    "    stimulus.Frame,\n",
    "    stimulus.MonetFrame,\n",
    "    stimulus.TrippyFrame,\n",
    "    stimulus.ColorFrameProjector,\n",
    "]\n",
    "imgsize = (144,256)\n",
    "response_integration_window = 0.5 # seconds\n",
    "response_integration_offset = 0.05 # seconds\n",
    "drop_invalid_behavior=True\n",
    "include_behavior=True\n",
    "stack_coordinates=False\n",
    "gamma_correction=False\n",
    "trace_tolerance = 0.01\n",
    "behavior_filter = 'boxcar'\n",
    "neuro_trace_filter = 'boxcar'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1b980",
   "metadata": {},
   "source": [
    "## Stimulus Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98a81d9",
   "metadata": {},
   "source": [
    "Images are extracted from stimulus.StaticImage.Image, where they are stored as 144x256 at uint8 resolution.  They are resized to 144x256 using cv2.INTER_AREA rescaling and retyped to float32.  \n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/frames.py#L17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0a4811d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading frames: 5100it [00:55, 91.44it/s]            \n"
     ]
    }
   ],
   "source": [
    "# images = (stimulus.StaticImage.Image & (stimulus.Frame & \n",
    "#                                         (stimulus.Trial & nda.ScanInclude & _valid_types))).fetch('image')\n",
    "# display(set([np.shape(i) for i in images]))\n",
    "# display(set([i.dtype for i in images]))\n",
    "\n",
    "all_conditions = (stimulus.Condition & (stimulus.Trial & scan_key & _valid_types)).fetch(\"KEY\")\n",
    "\n",
    "frames,frame_types = {},{}\n",
    "skipped = 0\n",
    "for cond_key in tqdm(all_conditions, total=len(all_conditions[0]), desc=\"Loading frames\"):\n",
    "    frame = (stimulus.StaticImage.Image & (stimulus.Frame & cond_key)).fetch1(\"image\")\n",
    "    frame_type = \"stimulus.Frame\"\n",
    "    \n",
    "    condition_hash = cond_key[\"condition_hash\"]\n",
    "    frames[condition_hash] = cv2.resize(frame, imgsize[::-1], interpolation=cv2.INTER_AREA).astype(np.float32)\n",
    "    frame_types[condition_hash] = frame_type\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c590a8",
   "metadata": {},
   "source": [
    "## utility functions\n",
    " https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/preprocessing.py#L4\n",
    " \n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/traces.py#L13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b03cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1d interpolation to fill nan values in trace, retaining nan gaps > len = preserve_gap (defaults to len 0)\n",
    "def fill_nans(x, preserve_gap=None):\n",
    "    \"\"\"\n",
    "    :param x:  1D array  -- will\n",
    "    :return: the array with nans interpolated\n",
    "    The input argument is modified.\n",
    "    \"\"\" \n",
    "    # check if input argument indicate that large gaps should be kept in \"keep\" variable\n",
    "    if preserve_gap is not None:\n",
    "        assert preserve_gap % 2 == 1, \"can only efficiently preserve odd gaps\"\n",
    "        # convolves np.isnan(trace) with preserve_gap length, to enumerage gap length centered on that position\n",
    "        # values outside edges assumed nonnan\n",
    "        # identifies position in center of gaps of length >= preserve gap\n",
    "        # convolves with preserve_gap length, to keep all positions within range of centers located within\n",
    "        # gap of length >= preserve gap\n",
    "        keep = np.convolve(\n",
    "            np.convolve(1 * np.isnan(x), np.ones(preserve_gap), mode=\"same\")\n",
    "            == preserve_gap,\n",
    "            np.ones(preserve_gap, dtype=bool),\n",
    "            mode=\"same\",\n",
    "        )\n",
    "    else:\n",
    "        # otherwise, keep vector set to all zeros\n",
    "        keep = np.zeros(len(x), dtype=bool)\n",
    "\n",
    "    # identify nan positions\n",
    "    nans = np.isnan(x)\n",
    "\n",
    "    # if all nans, set all trace values to zero\n",
    "    # otherwise linearly interpolate values at all nan positions from surrounding non-nan positions\n",
    "    # assumes regular sample intervals?\n",
    "    x[nans] = (\n",
    "        0\n",
    "        if nans.all()\n",
    "        else np.interp(nans.nonzero()[0], (~nans).nonzero()[0], x[~nans])\n",
    "    )\n",
    "    \n",
    "    # reset trace values at keep positions to nan\n",
    "    x[keep] = np.nan\n",
    "    \n",
    "    # return modified trace\n",
    "    return x\n",
    "\n",
    "def adjust_trace_len(traces, frame_times):\n",
    "    \"\"\"\n",
    "    Adjust neural traces to the same length\n",
    "    Args:\n",
    "        traces:        np.array of traces\n",
    "        frame_times:   frametimes corresponding to the traces 1d array\n",
    "    Returns: traces and frametimes shortened to the same length\n",
    "    \"\"\"\n",
    "    trace_len, nframes = traces.shape[1], frame_times.shape[1]\n",
    "    if trace_len < nframes:\n",
    "        frame_times = frame_times[:, :trace_len]\n",
    "    elif trace_len > nframes:\n",
    "        traces = traces[:, :nframes]\n",
    "    return traces, frame_times\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d578568e",
   "metadata": {},
   "source": [
    "## Get neuronal traces\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/traces.py#L115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e7cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsample from scan depth times (nframes * ndepths) to scan frame times \n",
    "ndepth = len(dj.U('z') & (meso.ScanInfo.Field & scan_key))\n",
    "frame_times = (stimulus.Sync & scan_key).fetch1(\"frame_times\").squeeze()[::ndepth]\n",
    "\n",
    "# restrict to masks classified as soma\n",
    "soma = meso.MaskClassification.Type() & dict(type=\"soma\")\n",
    "\n",
    "# fetch traces, delays, and keys\n",
    "spikes = (dj.U(\"field\", \"channel\") * meso.Activity.Trace * meso.ScanSet.UnitInfo * meso.ScanSet.Unit & soma & scan_key)\n",
    "\n",
    "traces, ms_delay, trace_keys = spikes.fetch(\"trace\", \"ms_delay\", dj.key, \n",
    "                                            order_by=\"animal_id, session, scan_idx, unit_id\")\n",
    "\n",
    "# stack traces, linearly interpolate across all nans (no nan gap preservation)\n",
    "traces = np.vstack([fill_nans(tr.astype(np.float32)).squeeze() for tr in traces])\n",
    "\n",
    "# correct delay from ms to s\n",
    "delay = np.fromiter(ms_delay / 1000, dtype=np.float)\n",
    "\n",
    "# create frame times vector for each trace, including estimated delay per trace\n",
    "frame_times_mat = delay[:, None] + frame_times[None, :]\n",
    "\n",
    "# clip to same trace length\n",
    "traces, frame_times_mat = adjust_trace_len(traces, frame_times_mat)\n",
    "\n",
    "# find beginning and end of scan frame times\n",
    "ftmin,ftmax = frame_times_mat.min(),frame_times_mat.max()\n",
    "\n",
    "# create complete trace keys for all traces\n",
    "trace_keys = [dict(scan_key, **trace_key) for trace_key in trace_keys]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539ae4fb",
   "metadata": {},
   "source": [
    "## Get neuron anatomical info\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/traces.py#L131"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e39122c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Get unit info: 100%|██████████| 8684/8684 [00:22<00:00, 392.33it/s]\n"
     ]
    }
   ],
   "source": [
    "rel = (\n",
    "        fuse.Activity.Trace.proj(\"animal_id\", \"session\", \"scan_idx\", \"unit_id\")\n",
    "        * anatomy.AreaMembership\n",
    "        * anatomy.LayerMembership\n",
    "        * meso.ScanSet.UnitInfo\n",
    "        & scan_key\n",
    "    )\n",
    "\n",
    "infos = []\n",
    "for trace_key in tqdm(trace_keys, desc=\"Get unit info\"):\n",
    "    info = (rel & trace_key).fetch(\n",
    "        \"brain_area\",\n",
    "        \"layer\",\n",
    "        \"animal_id\",\n",
    "        \"session\",\n",
    "        \"scan_idx\",\n",
    "        \"unit_id\",\n",
    "        \"um_x\",\n",
    "        \"um_y\",\n",
    "        \"um_z\",\n",
    "    )\n",
    "    infos.append((info))\n",
    "areas, layers, animal_ids, sessions, scan_idxs, unit_ids, x, y, z = map(\n",
    "        np.array, zip(*infos)\n",
    "    )\n",
    "cell_motor_coordinates = np.c_[x, y, z]\n",
    "assert len(np.unique(unit_ids)) == len(unit_ids), \"unit_ids are not unique\"\n",
    "\n",
    "neuron_info = dict(\n",
    "                    unit_ids=unit_ids.astype(np.uint16).squeeze(),\n",
    "                    animal_ids=animal_ids.astype(np.uint16).squeeze(),\n",
    "                    sessions=sessions.astype(np.uint8).squeeze(),\n",
    "                    scan_idx=scan_idxs.astype(np.uint8).squeeze(),\n",
    "                    layer=layers.astype(str).squeeze(),\n",
    "                    area=areas.astype(str).squeeze(),\n",
    "                    cell_motor_coordinates=cell_motor_coordinates.squeeze(),\n",
    "                  )        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbc012",
   "metadata": {},
   "source": [
    "## get stimulus onsets\n",
    "\n",
    "follows neuron trace fetch in order to remove trials with flip times outside the range of scan frame times.\n",
    "\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/stimulus.py#L129"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec9a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find trials affiliated with this scan\n",
    "# ExcludedTrial contains trials with flip number other than 3, which is redundant with below assertion\n",
    "targets = (stimulus.Condition & scan_key) * (stimulus.Trial & scan_key) - neuro_data.ExcludedTrial\n",
    "flip_times, trial_keys = targets.fetch(\"flip_times\", \"KEY\", order_by=\"condition_hash\")\n",
    "flip_times = [ft.squeeze() for ft in flip_times]\n",
    "\n",
    "# mark trials occuring before the beginning of the scan or after the end of the scan as invalid\n",
    "valid_trials = np.array([ft.min() >= ftmin and ft.max() <= ftmax for ft in flip_times], dtype=bool)\n",
    "if not np.all(valid_trials):\n",
    "    non_valid = (~valid_trials).sum()\n",
    "    print(f\"Dropping {non_valid} trials with dropped frames or flips outside the recording interval\")\n",
    "    \n",
    "# remove flip times and trial keys from invalid trials\n",
    "flip_times = list(compress(flip_times, valid_trials))\n",
    "trial_keys = [dict(scan_key, **trial_key) for trial_key in compress(trial_keys, valid_trials)]\n",
    "\n",
    "# assert predictable number of flips per trial (expected 2-3, expected all one size)\n",
    "n_ft = np.unique([ft.size for ft in flip_times])\n",
    "assert len(n_ft) == 1, \"Found inconsistent number of fliptimes\"\n",
    "n_ft = int(n_ft)\n",
    "assert n_ft in (2, 3), \"Cannot deal with {} flip times\".format(n_ft)\n",
    "\n",
    "# stack flip times such that trials are rows, column correspond to clear flip, onset_flip\n",
    "stimulus_onset = np.vstack(flip_times)\n",
    "\n",
    "# sort trials by first flip time\n",
    "ft = stimulus_onset[np.argsort(stimulus_onset[:, 0])]\n",
    "\n",
    "# reduce to only the column containing the second flip, indicating the blank end and \n",
    "# onset of imagenet image presentation\n",
    "stimulus_onsets = stimulus_onset[:, 1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95416011",
   "metadata": {},
   "source": [
    "## Custom Spline for interpolation into traces containing nans\n",
    "\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/splines.py#L7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "352baa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaNSpline(InterpolatedUnivariateSpline):\n",
    "    def __init__(self, x, y, **kwargs):\n",
    "        \n",
    "        # find nans in x or y\n",
    "        xnan = np.isnan(x)\n",
    "        ynan = np.isnan(y)\n",
    "        \n",
    "        # weights = nan in x or y\n",
    "        w = xnan | ynan  # get nans\n",
    "        \n",
    "        # convert to arrays\n",
    "        x, y = map(np.array, [x, y])  # copy arrays\n",
    "        \n",
    "        # set y at ynans to zero\n",
    "        y[ynan] = 0\n",
    "        \n",
    "        # set x at xnans to interpolate between known x values\n",
    "        x[xnan] = np.interp(np.where(xnan)[0], np.where(~xnan)[0], x[~xnan])\n",
    "        \n",
    "        # call InterpolatedUnivariateSpline init\n",
    "        # interpolate with passed kwargs, including k=spline_degree and ext = extrapolation mode beyond boundaries\n",
    "        # only pass positions that are not nan in x or y (effectively set weight of nan positions to zero)\n",
    "        super().__init__(x[~w], y[~w], **kwargs)  \n",
    "\n",
    "        # create interpolator to linearly interpolate x/y nans at any x position (1 = nan)\n",
    "        self.nans = interp1d(x, 1 * w, kind='linear')\n",
    "\n",
    "    def __call__(self, x, **kwargs):\n",
    "        # instantiate zero weight vectors\n",
    "        ret = np.zeros_like(x)\n",
    "        newnan = np.zeros_like(x)\n",
    "\n",
    "        # mark current nan positions in x vector with ones\n",
    "        old_nans = np.isnan(x)\n",
    "        newnan[old_nans] = 1\n",
    "        \n",
    "        # interpolate remaining positions from x/y nan interpolator\n",
    "        newnan[~old_nans] = self.nans(x[~old_nans])\n",
    "\n",
    "        # find any position that is currently nan or nan adjacent from the linear interpolator\n",
    "        idx = newnan > 0\n",
    "        \n",
    "        # set those positions to nan\n",
    "        ret[idx] = np.nan\n",
    "        \n",
    "        # all other positions, interpolate from the spline, \n",
    "        # passing kwargs (including k = spline degree,ext = extrapolation mode)\n",
    "        ret[~idx] = super().__call__(x[~idx], **kwargs)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0eb4f7",
   "metadata": {},
   "source": [
    "## Pupil Preprocessing\n",
    "\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/behavior.py#L64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b10d1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keeping some nans in the pupil location trace\n",
      "Keeping some nans in the pupil location trace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# convert loading parameters to function internal variable names\n",
    "duration = response_integration_window\n",
    "filter_type = behavior_filter\n",
    "DEEP_LAB_CUT = 2\n",
    "tracking_method = DEEP_LAB_CUT\n",
    "tolerance = trace_tolerance\n",
    "\n",
    "# sample point is midpoint of trial extraction window (takes offset into account)\n",
    "sample_point = (response_integration_offset + response_integration_window / 2)\n",
    "\n",
    "# find sample point for each stimulus trial by adding to onsets\n",
    "stimulus_mid_points = stimulus_onsets + sample_point\n",
    "\n",
    "# load radius and xy centers of fitted circles to the pupil using Deep Lab Cut\n",
    "r, center = (pupil.FittedPupil.Circle & \n",
    "             {**scan_key, \"tracking_method\": tracking_method}).fetch(\"radius\", \"center\", \n",
    "                                                                     order_by=\"frame_id\")\n",
    "# identify frames missing a fitted circle (indicated by nans)\n",
    "detectedFrames = ~np.isnan(r)\n",
    "\n",
    "# reformat centers from: [(x,y) or nan] to: [(x,y) or (nan,nan)]\n",
    "xy = np.full((len(r), 2), np.nan)\n",
    "xy[detectedFrames, :] = np.vstack(center[detectedFrames])\n",
    "\n",
    "# linearly interpolate across small nan gaps in x and y traces independently, \n",
    "# preserving nan gaps with length >= 3\n",
    "xy = np.vstack(map(partial(fill_nans, preserve_gap=3), xy.T))\n",
    "if np.any(np.isnan(xy)):\n",
    "    print(\"Keeping some nans in the pupil location trace\")\n",
    "\n",
    "# linearly interpolate across small nan gaps in radius trace, preserving nan gaps with length >= 3\n",
    "pupil_radius = fill_nans(r.squeeze(), preserve_gap=3)\n",
    "if np.any(np.isnan(pupil_radius)):\n",
    "    print(\"Keeping some nans in the pupil location trace\")\n",
    "radius = pupil_radius\n",
    "    \n",
    "# fetch the frame times of the eye camera in the behavior clock\n",
    "eye_time = (pupil.Eye() & scan_key).fetch1(\"eye_time\").squeeze()\n",
    "\n",
    "# fetch the scan frame times in the behavior clock\n",
    "behavior_clock = (stimulus.BehaviorSync & scan_key).fetch1('frame_times').squeeze()[::ndepth]\n",
    "\n",
    "# trim scan frame times in stimulus clock (frame times) and scan frame times in behavior clock (behavior_clock)\n",
    "# to be the same length, allowing interpolation between the two clocks. \n",
    "if len(frame_times) - len(behavior_clock) != 0:\n",
    "    assert (\n",
    "        abs(len(frame_times) - len(behavior_clock)) < 2\n",
    "    ), \"Difference bigger than 2 time points\"\n",
    "    l = min(len(frame_times), len(behavior_clock))\n",
    "    frame_times = frame_times[:l]\n",
    "    behavior_clock = behavior_clock[:l]\n",
    "\n",
    "# interpolation object for moving from frame times in stimulus clock to frame times in behavior clock\n",
    "# k = 1, degree of spline\n",
    "# ext = 3 -> extrapolation mode 3 'constant', return the boundary value\n",
    "fr2beh = NaNSpline(frame_times, behavior_clock, k=1, ext=3)\n",
    "\n",
    "\n",
    "# create boxcar trace for all stimulus mid points\n",
    "# -duration/2 to duration/2, step length = tolerance\n",
    "print(\"Extracting pupil signal using boxcar\")\n",
    "assert len(stimulus_mid_points.shape) == 1, \"stimulus_points need to be a 1d array\"\n",
    "dt = np.arange(-duration / 2, duration / 2, tolerance)[:, None]\n",
    "T = dt + stimulus_mid_points[None, :]\n",
    "\n",
    "# convert boxcar windows from stimulus clock to behavior clock\n",
    "T = fr2beh(T.ravel()).reshape(T.shape)\n",
    "\n",
    "# create function for interpolating boxcar windows into signal traces, \n",
    "# using nearest interpolation\n",
    "# then taking mean across boxcar period\n",
    "def filter(signal):\n",
    "    upsampler = interp1d(eye_time, signal, kind=\"nearest\")\n",
    "    return upsampler(T).mean(axis=0)\n",
    "\n",
    "# use to extract mean signal for all boxcar windows from the pupil radius, delta pupil radius, \n",
    "# and pupil center traces\n",
    "pupil = filter(radius)\n",
    "dpupil = filter(np.gradient(radius))\n",
    "center = np.vstack([filter(coord) for coord in xy])\n",
    "\n",
    "# for any trial containing a nan in one of the three pupil traces\n",
    "# set the values of that trial to -1 in all three traces\n",
    "# mark that trial as invalid in valid_eye vector\n",
    "# note, this is after linearly interpolating across nan gaps with length <3\n",
    "# and will include nans inherited from nearest neighboring positions due to interp1d\n",
    "valid = ~np.isnan(pupil + dpupil + center.sum(axis=0))\n",
    "if not np.all(valid):\n",
    "    print(\"Found {} NaN trials. Setting to -1\".format((~valid).sum()))\n",
    "    pupil[~valid] = -1\n",
    "    dpupil[~valid] = -1\n",
    "    center[:, ~valid] = -1\n",
    "pupil_center = center.T\n",
    "valid_eye = valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26bc728",
   "metadata": {},
   "source": [
    "## Treadmill Preprocessing\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/behavior.py#L137\n",
    "\n",
    "reuses interpolator / sample points from pupil processing above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd6f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch treadmill time and velocity for scan\n",
    "treadmill_time,treadmill_signal = (treadmill.Treadmill() & scan_key).fetch1(\"treadmill_time\", \"treadmill_vel\")\n",
    "treadmill_time,treadmill_signal = treadmill_time.squeeze(), treadmill_signal.squeeze()\n",
    "\n",
    "# create boxcar trace for all stimulus mid points\n",
    "# -duration/2 to duration/2, step length = tolerance\n",
    "print(\"Extracting treadmill signal using boxcar\")\n",
    "assert len(stimulus_mid_points.shape) == 1, \"stimulus_points need to be a 1d array\"\n",
    "dt = np.arange(-duration / 2, duration / 2, tolerance)[:, None]\n",
    "T = dt + stimulus_mid_points[None, :]\n",
    "\n",
    "# convert boxcar windows from stimulus clock to behavior clock\n",
    "T = fr2beh(T.ravel()).reshape(T.shape)\n",
    "\n",
    "# interpolate boxcar windows into treadmill_signal using nearest interpolation\n",
    "# then take mean across boxcar period\n",
    "upsampler = interp1d(treadmill_time, treadmill_signal, kind=\"nearest\")\n",
    "tm = upsampler(T).mean(axis=0)\n",
    "\n",
    "# for any trial containing a nan in the mean treadmill value\n",
    "# set the value of that trial to -1\n",
    "# mark this trial as invalid in valid_treadmill vector\n",
    "# note that treadmill values are not preprocessed by nan interpolation utility fill_nans\n",
    "valid = ~np.isnan(tm)\n",
    "if not np.all(valid):\n",
    "    warn(\"Found {} NaN trials. Setting to -1\".format((~valid).sum()))\n",
    "    tm[~valid] = -1\n",
    "valid_treadmill = valid\n",
    "\n",
    "# rename tread_mill_values\n",
    "tread_mill_values = tm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265f0ea2",
   "metadata": {},
   "source": [
    "## Trial Exclusion for Behavior\n",
    "\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/imagenet.py#L200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9311a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create vector, default all trials included\n",
    "valid_signals = np.ones(len(stimulus_onsets), dtype=bool)\n",
    "\n",
    "# join with valid trials due to nan detection in either pupil or treadmill traces\n",
    "valid_signals &= valid_eye\n",
    "valid_signals &= valid_treadmill\n",
    "\n",
    "# join behavior as a single matrix\n",
    "behavior = np.c_[pupil, dpupil, tread_mill_values]\n",
    "\n",
    "# if any invalid trials, and flag for dropping invalid trials due to behavior = True\n",
    "if not np.all(valid_signals) and drop_invalid_behavior:\n",
    "    non_valid = (~valid_signals).sum()\n",
    "    print(f\"Dropping {non_valid} trials because of missing signals in behavioral recordings\")\n",
    "    \n",
    "    # remove invalid trials from stimulus onsets, trial_keys, behavior traces, pupil center traces, and \n",
    "    # valid trial tracker \n",
    "    stimulus_onsets = stimulus_onsets[valid_signals]\n",
    "    trial_keys = list(compress(trial_keys, valid_signals))\n",
    "    if include_behavior:\n",
    "        behavior = behavior[valid_signals]\n",
    "        pupil_center = pupil_center[valid_signals]\n",
    "        valid_signals = valid_signals[valid_signals]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a868d4c8",
   "metadata": {},
   "source": [
    "## Complete Trace Preprocessing\n",
    "\n",
    "https://github.com/sinzlab/nexport/blob/80d66912bcb6785f8c54ec596e70de4656ac964a/nexport/exporters/utils/traces.py#L74\n",
    "\n",
    "Follow behavior preprocessing to allow for removal of trials with nans in behavior traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae86a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample point is midpoint of trial extraction window (takes offset into account)\n",
    "sample_point = (response_integration_offset + response_integration_window / 2)\n",
    "\n",
    "# find sample point for each stimulus trial by adding to onsets\n",
    "stimulus_points = stimulus_onsets + sample_point\n",
    "\n",
    "sampling_period = response_integration_window\n",
    "print(f\"Generating lowpass filters to {1/sampling_period}Hz\")\n",
    "\n",
    "responses = []\n",
    "assert len(stimulus_points.shape) == 1, \"stimulus_points need to be a 1d array\"\n",
    "for frame_time, trace in tqdm(zip(frame_times_mat, traces), desc=\"Sampling traces\", total=len(traces)):\n",
    "    # Upsampling and convolution with boxcar is too slow. Instead we \"upsample\" time at the size of the\n",
    "    # boxcar symmetrically around zero, add that window to all sampling points, extract the nearest\n",
    "    # interpolation of the original signal around those points and average across each window. This is orders\n",
    "    # of magnitude faster\n",
    "\n",
    "    # create boxcar trace for all stimulus mid points\n",
    "    # -duration/2 to duration/2, step length = tolerance\n",
    "    dt = np.arange(-sampling_period / 2, sampling_period / 2, tolerance)[:, None]\n",
    "    T = dt + stimulus_points[None, :]\n",
    "    \n",
    "    # interpolate boxcar windows into trace signals using nearest interpolation\n",
    "    # then take mean across boxcar period\n",
    "    upsampler = interp1d(frame_time, trace, kind=\"nearest\")\n",
    "    responses.append(upsampler(T).mean(axis=0))\n",
    "responses = np.vstack(responses).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdfa72a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df2e662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd4525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ff723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65d148e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf482d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238818e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41cd0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341c0524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de44aef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324f5da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84affa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "{'img_size': (144, 256),\n",
    "  'response_integration_window': 0.5,\n",
    "  'response_integration_offset': 0.05,\n",
    "  'include_behavior': True,\n",
    "  'neuro_trace_filter': 'boxcar',\n",
    "  'behavior_filter': 'boxcar',\n",
    "  'stack_coordinates': False,\n",
    "  'gamma_correction': False,\n",
    "  'trace_tolerance': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd6f91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from itertools import compress\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from .. import logger as log\n",
    "from ..schemas.bcm.stim_info import ConditionTier\n",
    "from . import Exporter\n",
    "from .utils import DEEP_LAB_CUT\n",
    "from .utils.behavior import extract_pupil, extract_treadmill\n",
    "from .utils.frames import load_frame_and_type, rescale_frame, stack_frames\n",
    "from .utils.statistics import run_stats\n",
    "from .utils.stimulus import (\n",
    "    get_trialkeys_and_stimulus_onsets,\n",
    "    get_extra_stimulus_info,\n",
    "    get_album,\n",
    ")\n",
    "from .utils.storage import save_dict_to_hdf5, save_to_folder, zip_dir\n",
    "from .utils.traces import (\n",
    "    extract_traces,\n",
    "    get_neuron_info,\n",
    "    get_stack_coordinates,\n",
    "    get_xmatches,\n",
    "    extract_neural_responses,\n",
    ")\n",
    "from .utils.verification import (\n",
    "    pupil_present,\n",
    "    treadmill_present,\n",
    "    album_present,\n",
    "    behavior_synced,\n",
    "    area_and_layers_consistent,\n",
    "    has_motor_coordinates,\n",
    "    has_stack_coordinates,\n",
    "    has_xmatch_coordinates,\n",
    "    has_traces,\n",
    "    has_gamma_correction,\n",
    "    soma_classiafication_present,\n",
    "    has_condition_tier_assigned,\n",
    ")\n",
    "from ..cajal import stimulus, multi_mei\n",
    "\n",
    "\n",
    "class ImageNet(Exporter):\n",
    "    _base_stimulus = None\n",
    "\n",
    "    _valid_types = [\n",
    "        stimulus.Frame,\n",
    "        stimulus.MonetFrame,\n",
    "        stimulus.TrippyFrame,\n",
    "        stimulus.ColorFrameProjector,\n",
    "    ]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_size=(36, 64),\n",
    "        response_integration_window=0.5,  # in seconds\n",
    "        response_integration_offset=0.05,  # in seconds\n",
    "        include_behavior=True,\n",
    "        tracking_method=DEEP_LAB_CUT,  # TODO: replace with more concrete values/string\n",
    "        neuro_trace_filter=\"hamming\",\n",
    "        behavior_filter=\"hamming\",\n",
    "        stack_coordinates=False,\n",
    "        gamma_correction=False,\n",
    "        trace_tolerance=0.01,\n",
    "        drop_invalid_behavior=True,\n",
    "    ):\n",
    "        self.img_size = img_size\n",
    "        self.response_integration_window = response_integration_window\n",
    "        self.response_integration_offset = response_integration_offset\n",
    "        self.include_behavior = include_behavior\n",
    "        self.tracking_method = tracking_method\n",
    "        self.behavior_filter = behavior_filter\n",
    "        self.stack_coordinates = stack_coordinates\n",
    "        self.gamma_correction = gamma_correction\n",
    "        self.neuro_trace_filter = neuro_trace_filter\n",
    "        self.trace_tolerance = trace_tolerance\n",
    "        self.drop_invalid_behavior = drop_invalid_behavior\n",
    "        self._cache = {}\n",
    "\n",
    "    def filename(self, scan_key, confighash):\n",
    "        \"\"\"\n",
    "        Get the filename corresponding to the particular scan and exporter config as indicated by the `confighash`\n",
    "        Args:\n",
    "            scan_key (dict): A dictionary corresponding to DataJoint key for a single Scan table entry\n",
    "            confighash (str): A string to uniquely identify a specific exporter configuration\n",
    "        Returns:\n",
    "            str: The filename to adequately identify the particular scan to be exported with this particular exporter\n",
    "        \"\"\"\n",
    "        return \"static{animal_id}-{session}-{scan_idx}-{exporter_name}-{confighash}\".format(\n",
    "            exporter_name=self.__class__.__name__, confighash=confighash, **scan_key\n",
    "        )\n",
    "\n",
    "    def clean_cache(self):\n",
    "        self._cache = {}\n",
    "\n",
    "    def to_hdf5(self, data, scan_key, confighash, overwrite=False, base_path=\".\"):\n",
    "        filename = self.filename(scan_key, confighash) + \".h5\"\n",
    "        full_path = Path(base_path) / filename\n",
    "        if full_path.exists() and not overwrite:\n",
    "            raise FileExistsError(f\"File {full_path} already exists\")\n",
    "        save_dict_to_hdf5(data, full_path)\n",
    "\n",
    "    def to_folder(\n",
    "        self,\n",
    "        data,\n",
    "        scan_key,\n",
    "        confighash,\n",
    "        overwrite=False,\n",
    "        zip=True,\n",
    "        base_path=\".\",\n",
    "    ):\n",
    "        dirname = self.filename(scan_key, confighash)\n",
    "        full_path = Path(base_path) / dirname\n",
    "        if full_path.exists() and not overwrite:\n",
    "            raise FileExistsError(f\"Path {full_path} already exists\")\n",
    "        else:\n",
    "            full_path.mkdir(exist_ok=True, parents=True)\n",
    "        save_to_folder(\n",
    "            data, full_path, overwrite=overwrite, include_behavior=self.include_behavior\n",
    "        )\n",
    "        if zip:\n",
    "            zip_dir(full_path.with_suffix(\".zip\"), full_path)\n",
    "            shutil.rmtree(full_path)\n",
    "\n",
    "    def verify(self, scan_key):\n",
    "        if self.include_behavior:\n",
    "            behavior_synced(scan_key)\n",
    "            pupil_present(scan_key)\n",
    "            treadmill_present(scan_key)\n",
    "\n",
    "        has_condition_tier_assigned(scan_key, valid_conditions=self._valid_types)\n",
    "        # Adapt to work with all valid stimulus types\n",
    "        album_present(scan_key, self._base_stimulus)\n",
    "        has_motor_coordinates(scan_key)\n",
    "        area_and_layers_consistent(scan_key)\n",
    "        has_traces(scan_key)\n",
    "        soma_classification_present(scan_key)\n",
    "\n",
    "        if self.stack_coordinates:\n",
    "            has_stack_coordinates(scan_key)\n",
    "\n",
    "        if self.gamma_correction:\n",
    "            has_gamma_correction(scan_key)\n",
    "\n",
    "    def export(self, scan_key):\n",
    "        self.clean_cache()\n",
    "        final_dataset = {}\n",
    "        # --- Preprocess frames\n",
    "        frames = {}\n",
    "        frame_types = {}\n",
    "        frame_tiers = {}\n",
    "\n",
    "        skipped = 0\n",
    "        all_conditions = (\n",
    "            stimulus.Condition * ConditionTier\n",
    "            & (stimulus.Trial & scan_key & self._valid_types)\n",
    "        ).fetch(\"KEY\", \"tier\")\n",
    "        for cond_key, tier in tqdm(\n",
    "            zip(*all_conditions),\n",
    "            total=len(all_conditions[0]),\n",
    "            desc=\"Loading frames\",\n",
    "        ):\n",
    "            frame, frame_type = load_frame_and_type(cond_key)\n",
    "\n",
    "            condition_hash = cond_key[\"condition_hash\"]\n",
    "            frames[condition_hash] = rescale_frame(frame, self.img_size)\n",
    "            frame_tiers[condition_hash] = tier\n",
    "            frame_types[condition_hash] = frame_type\n",
    "\n",
    "        # --- Preprocess responses\n",
    "        # get middle of extraction window\n",
    "        sample_point = (\n",
    "            self.response_integration_offset + self.response_integration_window / 2\n",
    "        )\n",
    "        log.info(\n",
    "            f\"Sampling neural responses at {self.response_integration_window}s intervals\"\n",
    "        )\n",
    "\n",
    "        # get interpolation of neurotraces along with key for each trace and min/max frametimes\n",
    "        traces, trace_keys, frame_times, ftmin, ftmax = extract_traces(scan_key)\n",
    "        trace_keys = [dict(scan_key, **trace_key) for trace_key in trace_keys]\n",
    "        self._cache[\"trace_keys\"] = trace_keys\n",
    "\n",
    "        neuron_info = get_neuron_info(scan_key, trace_keys)\n",
    "        if self.stack_coordinates:\n",
    "            log.info(\"Adding multi-unit matches and coordinates\")\n",
    "            stack_info = get_stack_coordinates(scan_key, trace_keys)\n",
    "            neuron_info.update(**stack_info)\n",
    "\n",
    "        # get stimulus fliptimes and keys for single trials\n",
    "        trial_keys, stimulus_onsets = get_trialkeys_and_stimulus_onsets(\n",
    "            scan_key=scan_key, ftmin=ftmin, ftmax=ftmax, restr=ConditionTier\n",
    "        )\n",
    "\n",
    "        # --- extract eye values\n",
    "        valid_signals = np.ones(len(stimulus_onsets), dtype=bool)\n",
    "        if self.include_behavior:\n",
    "            pupil, dpupil, pupil_center, valid_eye = extract_pupil(\n",
    "                scan_key,\n",
    "                stimulus_onsets + sample_point,\n",
    "                self.response_integration_window,\n",
    "                self.behavior_filter,\n",
    "                self.tracking_method,\n",
    "                tolerance=self.trace_tolerance,\n",
    "            )\n",
    "            valid_signals &= valid_eye\n",
    "\n",
    "            # --- extract treadmill values\n",
    "            tread_mill_values, valid_treadmill = extract_treadmill(\n",
    "                scan_key,\n",
    "                stimulus_onsets + sample_point,\n",
    "                self.response_integration_window,\n",
    "                self.behavior_filter,\n",
    "                tolerance=self.trace_tolerance,\n",
    "            )\n",
    "            valid_signals &= valid_treadmill\n",
    "            behavior = np.c_[pupil, dpupil, tread_mill_values]\n",
    "\n",
    "        if not np.all(valid_signals) and self.drop_invalid_behavior:\n",
    "            non_valid = (~valid_signals).sum()\n",
    "            log.info(\n",
    "                f\"Dropping {non_valid} trials because of missing signals in behavioral recordings\"\n",
    "            )\n",
    "            stimulus_onsets = stimulus_onsets[valid_signals]\n",
    "            trial_keys = list(compress(trial_keys, valid_signals))\n",
    "            if self.include_behavior:\n",
    "                behavior = behavior[valid_signals]\n",
    "                pupil_center = pupil_center[valid_signals]\n",
    "                valid_signals = valid_signals[valid_signals]\n",
    "\n",
    "        hashes = np.array(\n",
    "            list(map(itemgetter(\"condition_hash\"), trial_keys)), dtype=str\n",
    "        )\n",
    "        tiers = np.array([frame_tiers[h] for h in hashes]).astype(str)\n",
    "        types = np.array([frame_types[h] for h in hashes]).astype(str)\n",
    "        trial_idxs = np.array(list(map(itemgetter(\"trial_idx\"), trial_keys)), dtype=int)\n",
    "\n",
    "        responses = extract_neural_responses(\n",
    "            stimulus_onsets + sample_point,\n",
    "            frame_times,\n",
    "            traces,\n",
    "            self.response_integration_window,\n",
    "            filter_type=self.neuro_trace_filter,\n",
    "            tolerance=self.trace_tolerance,\n",
    "        )\n",
    "\n",
    "        images = stack_frames([frames[ch] for ch in hashes])\n",
    "        row_info = get_extra_stimulus_info(scan_key, hashes, trial_idxs, types)\n",
    "\n",
    "        row_info[\"album\"] = get_album(scan_key, trial_keys, self._base_stimulus)\n",
    "\n",
    "        # gamma correction\n",
    "        if self.gamma_correction:\n",
    "            log.info(\"Applying gamma correction\")\n",
    "            # This won't work as `get_fs` is not defined anywhere\n",
    "            # TODO: fix this\n",
    "            f, f_inv = (multi_mei.ClosestCalibration & scan_key).get_fs()\n",
    "            images = f(images)\n",
    "\n",
    "        # --- compute statistics\n",
    "        response_statistics = run_stats(\n",
    "            selector=lambda ix: responses[ix],\n",
    "            types=types,\n",
    "            ix=(tiers == \"train\" if \"train\" in tiers else tiers == \"probe\"),\n",
    "            axis=0,\n",
    "        )\n",
    "        input_statistics = run_stats(\n",
    "            selector=lambda ix: images[ix],\n",
    "            types=types,\n",
    "            ix=(tiers == \"train\" if \"train\" in tiers else tiers == \"probe\"),\n",
    "        )\n",
    "\n",
    "        statistics = dict(images=input_statistics, responses=response_statistics)\n",
    "\n",
    "        if self.include_behavior:\n",
    "            # ---- include statistics\n",
    "            behavior_statistics = run_stats(\n",
    "                selector=lambda ix: behavior[ix],\n",
    "                types=types,\n",
    "                ix=(tiers == \"train\" if \"train\" in tiers else tiers == \"probe\"),\n",
    "                axis=0,\n",
    "            )\n",
    "            eye_statistics = run_stats(\n",
    "                selector=lambda ix: pupil_center[ix],\n",
    "                types=types,\n",
    "                ix=(tiers == \"train\" if \"train\" in tiers else tiers == \"probe\"),\n",
    "                axis=0,\n",
    "            )\n",
    "\n",
    "            statistics[\"behavior\"] = behavior_statistics\n",
    "            statistics[\"pupil_center\"] = eye_statistics\n",
    "\n",
    "        # TODO: Gamma correction\n",
    "        # --- assemble final dataset\n",
    "        final_dataset.update(\n",
    "            images=images,\n",
    "            trial_idxs=trial_idxs,\n",
    "            responses=responses,\n",
    "            hashes=hashes,\n",
    "            tiers=tiers,\n",
    "            types=types,\n",
    "            item_info=row_info,\n",
    "            neurons=neuron_info,\n",
    "            statistics=statistics,\n",
    "        )\n",
    "        if self.include_behavior:\n",
    "            final_dataset[\"behavior\"] = behavior\n",
    "            final_dataset[\"pupil_center\"] = pupil_center\n",
    "            final_dataset[\"valid_behavior\"] = valid_signals\n",
    "\n",
    "        return final_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
